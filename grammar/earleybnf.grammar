(*         Earley BNF grammar         *)
(*         ==================         *)

(* This self-documenting grammar is written in a literate style where
paragraphs of text (such as this one) are ignored by the compiler compiler,
while production rules like the following are interpreted correctly. *)

input ::= _ grammar _ => \2

(* The first production rule is considered the default start state.  The entire
grammar consists of production rules like the above or comments (like this
paragraph).  The following is how these alternatives are defined. *)

grammar ::= production
grammar ::= grammar _ production => [\1..., \3]

(* Production rules are defined with `::=` (details below).  The contents of
each rule are token matchers or nonterminals which are indexed into a virtual
list.  The `=>` operator provides a way to define postprocessing on the list
contents, where each list item is referenced with (0-indexed) \-notation. *)

(* Not all items being matched are important, such as the optional spacing
indicated by the `_` rule.  This grammar includes two convenience rules for
matching an optional space and matching at-least-one spacing.  The value for
matched spacing is ignored, such that the propagated list only contains comment
nodes, effectively discarding any spacing (including newlines). *)

_ ::= __?

__ ::= __? SPACING => [ \1... ]
     | __? COMMENT => [ \1..., Comment{text: \2} ]

(* Variations on this are possible, such as requiring at least two newlines. *)


SPACING ::= /\s+/m

(* Spacing is straightforward with the above pattern.  Comment parsing is a
little more involved, but still relatively shallow compared to other production
rules.  We are able to capture it with the following pattern. *)

COMMENT ::= /\(\*([^*]+|\*+[^)])*\*+\)/m => \1

(* In this way, any block of text that begins with a '(' followed by a '*' will
be composed as a comment until the next appearance of '*' and ')'.  This rule is
matched at the tokenizer level as all text between these symbol pairs is thus
consumed, so it is difficult to spell them out more deliberately here.  Some
earlier experiments with this format used leading spaces to distinguish prose
paragraphs from grammar rules, but it created too much additional complexity in
the grammar definition to be considered worth pursuing.  Instead, this block-
comment denotation was used, borrowed from the formal definition of EBNF. *)

(* Note that these token rules can also have post processing, similar to the
production rules, whereby \0 refers to the entire pattern, \1 to the first match
group, \2 the second match group, et cetera.  To make things consistent for
post-processing, match patterns like this always appear on their own, and any
postprocessing should always emit a string (or list of bytes equivalent).  The
grammar restricts this isolation of references for pattern definitions, it is
sufficient to cover all useful cases, and deep references from parents are valid
if they only refer to the numerical offset (no property names or named groups).
However, as with state projection, enclosing rules can only select from what is
exposed by the child rules, otherwise there may be ambiguity w.r.t. references.
*)

(* Tokens may also be represented by string literals provided inline within a
production rule. The lexer will be given a temporary token with a name that
equals its image (e.g. a token type is created for `::=` in the following rule).
In this way a token is defined only once even if it is repeated in other rules,
and there is no added syntactical baggage for defining Token representations
explicitly except where terminals would be made clearer by naming them. *)

production ::=
	  WORD _ "::=" _ rule_body    => Rule{ name: \1, choices: \5 }
	| WORD _ "::=" _ pattern_body => Matcher{ \5..., name: \1 }

(* This rounds out the definition of the grammar at a high level, and we can now
focus on the phrasing of individual production rules.  One semantic detail, if a
rule or pattern name is repeated, they are considered multiple alternate paths
and may all satisfy the symbol represented by name shared by these alternatives.  
In the case that one is a production rule and another a token pattern, the
compiler will do its best to merge them as long as their produced types (string
or list) are consistent. *)


(* Grammar rules *)
(* ------------- *)

(*
   As mentioned above, a rule body may be a solitary pattern.  When phrased this
way, its post-processing captures the pattern and subgroups instead of Earley
states, allowing versatility in token specification without requiring another
sublanguage to define them (other than the regex pattern language, of course).
*)

pattern_body ::=
    PATTERN => Matcher{ pattern: \1 }
  | PATTERN _ "=>" _ postproc_ref => Matcher{ pattern: \1, post: \5 }

(* The pattern is phrased between forward-slash characters '/' and must occupy
only one line.  We do not attempt a full parse of the pattern here, we depend
on a sensible choice of regular expression matching in the compiler's env.  The
compilation of the pattern would be deferred until parsing happens by the code
generated by this grammar's compilation (or another grammar described in this
language), so the overhead of parsing the pattern here would be wasted as it is.
*)

PATTERN ::= /\/(?:\\.|[^\\\n])+?\/m?/

(* Otherwise, a rule body is one or more alternatives,
each a string, character class or reference to another production rule. *)

rule_body ::= parse_choice
	| rule_body _ "|" _ parse_choice => [\1..., \5]

(* Each alternative is either a sequence of token expressions by itself or with
additional post-processing when the '=>' operator is present. *)

parse_choice ::=
	  rule_expr => Choice{ tokens: \1 }
	| rule_expr _ "=>" _ postproc_atom => Choice{ tokens: \1, post: \5 }

(* Token sequencing is simple concatenation. *)

rule_expr ::= rule_atom
	| rule_expr _ rule_atom => [\1..., \3]

(* Each token representation may be a matcher or group or special Kleene group.
When subgroups are present in a rule, post-processing references for the outer
rule will consider the entire group as one set (thus a list, as though it were
a separate rule, which a temporary Grammar entry will be created for) and the
group's alternatives may each have their own postprocessing to express what gets
projected from the contents of the sub-expression. *)

rule_atom ::=
	  rule_matcher => \1
	| rule_matcher kleene_mod => Matcher{ \1..., kleene: \2 }
	| "(" _ rule_body _ ")" _ kleene_mod? => Expr{ tokens: \3, kleene: \7 }
  | "[" _ rule_body _ "]" => Expr{ tokens: \3, kleene: "?" }
  | "{" _ rule_body _ "}" => Expr{ tokens: \3, kleene: "*" }

(* Any word (symbolic name) is a reference to another rule in the grammar.
There may be a literal string for inlining token definitions (including symbols
and keywords), and the tokenizer will be informed about these and any character
classes.  Anything needing a regular expression must be defined as a rule by
itself.  This also enables projecting from a subgroup of the pattern easily. *)

rule_matcher ::=
	  WORD      => Matcher{ nonterm: \1 }
	| STRING    => Matcher{ literal: \1 }
	| CHARCLASS => Matcher{ pattern: \1 }

(* Groups may be 0-1, 0-or-more, or 1-or-more, expressed via Kleene symbols. *)

kleene_mod ::= [?*+]

(* A word may contain any alphanumeric characters but must begin with a letter
or the underscore.  It may even consist of only underscores or only one letter.
*)

WORD ::= /[A-Z_a-z][A-Z_a-z0-9]*/

(* Capture all characters between double-quotes.
*)

STRING ::= /"(\\["bfnrt\/\\]|\\u[a-fA-F0-9]{4}|[^"\\\n])*"/ => \1

(* Capture a range of characters and character instances within a common class.
*)

CHARCLASS ::= /\[(?:\\.|[^\\\n])+?\]/


(* Post-processing *)
(* --------------- *)

(* Here is where the grammar departs significantly from Extended Backus-Naur
Form (the original EBNF).  EarleyBNF is capable of expressing a post-processing
transformation in which tokens are selected, rearranged, and given structure.

If no postprocessing is given, it is equivalent to [ \1, \2, ...] (etc. to fit),
where each reference is to the ordering of states, and the entire list is 
equivalent to using the zero reference ('\0').  A rule always produces a list
by default, even for empty matching strings, so it is common to see `=> \1` as
the postprocessing clause, to extract the first (or only) term from the list.

For pattern tokens, the default is also \0, corresponding to the entire match.
Subgroups within a regular expression pattern can also be projected using the
same \-notation as states.  This allows for consistency between token and state
projection representations, while the actual return type for pattern values is
a string (corresponding to other token representations).
*)

postproc_atom ::=
    postproc_prop => \1
  | postproc_ref  => \1
  | postproc_list => \1
  | postproc_record => \1

(*
   The simplest post-processing is a single reference to a positional element.
In the case of token patterns this is the only valid postprocessing expression.
*)

postproc_ref ::= "\\" NUMBER => ItemProjection{ ref: \2 }

(*
   Numbers are always positive integers, or zero.  To avoid confusion with octal
values in other languages, and to avoid unnecessary leading zeroes, this pattern
is especially specific about which digit strings are allowed.
*)

NUMBER ::= /0|[1-9][0-9]*/

(*
   The projections for post-processing may include references to properties that
are defined within an object or list defined for the states of the current rule.
*)

postproc_prop ::= postproc_ref "." WORD => PropertyGetter{ref: \1, name: \3}
postproc_prop ::= postproc_prop "." WORD => PropertyGetter{ref: \1, name: \3}
postproc_prop ::= postproc_ref "." NUMBER => PropertyGetter{ref: \1, name: \3}
postproc_prop ::= postproc_prop "." NUMBER => PropertyGetter{ref: \1, name: \3}

(*
   Post-processing may construct a list or a record object from the spec given,
with a list able to expand the contents of another list (and a sufficiently
good compiler will see this for the append-to-slice operation that it is...).
*)

postproc_list ::= "[" _ postproc_items _ ","? _ "]"
  => ListProjection{ values: \3 }

postproc_items ::= postproc_item
postproc_items ::= postproc_items _ "," _ postproc_item => [\1..., \5]

(*
   Each item in the list may be a reference, a property of a reference,
an embedded structure like a list or record (explicitly defined or lifted from
the reference to a state in the associated rule).  It may also be an expansion
for the elements of a list at the indicated reference.  An AST validation pass
is needed to confirm that the type of the referred rule atom is indeed a list.
*)

postproc_item ::= postproc_atom => \1
                | postproc_ref "..." => ExpandList{ ref: \1.ref }

(*
A record is composed of a name (preceding the '{') and a list of attributes as
key-value association pairs.  Each key must conform to a naming word, the values
may be any PostProcessing-compatible values (postproc_atom).  It may contain an
expansion rule in its attributes in place of a key-value pair.  This assumes the
referred item being expanded is also a postproc_record that unifies with this
one.  A trailing comma after the last element is allowed and optional.
*)

(* Note: no space allowed between the type/name and the open curly brace. *)
postproc_record ::= word "{" _ postproc_keyvals _ ","? _ "}"
  => RecordProjection{ name: \1, attrs: \4 }

(* Multiple attributes are separated by a comma. *)

postproc_keyvals ::= postproc_kv
postproc_keyvals ::= postproc_keyvals _ "," _ postproc_kv => [\1..., \5]

(* Each (key, value) is colon-separated.  A reference expansion also needs to be
the same name/type as the record being expanded into, but that is checked in a
validation pass over the AST after construction. *)

postproc_kv ::= word _ ":" _ postproc_atom => KeyValue{ "key": \1, "value": \5 }
postproc_kv ::= postproc_ref "..." => ExpandRecord{ ref: \1.ref }
